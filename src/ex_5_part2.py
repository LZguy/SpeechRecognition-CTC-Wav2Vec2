# -*- coding: utf-8 -*-
"""ex_5_part2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sBTp9FylFwxoBsr5KespYiD3J9TpHbaa
"""

# #kenlm
# !git clone https://github.com/kpu/kenlm.git
# !mkdir -p kenlm/build
# !cd kenlm/build && cmake .. && make -j4
# !./kenlm/build/bin/lmplz -o 5 < train_transcription.txt > kenlm.arpa --discount_fallback
#
# !unzip /content/drive/MyDrive/EX5_updated -d /content/ #change it.
#
# # Install necessary packages
# !pip install datasets transformers torchaudio jiwer
# !pip install flashlight-text
# !pip install https://github.com/kpu/kenlm/archive/master.zip

import os
import re
import torch
import kenlm
import torchaudio
import numpy as np
import pandas as pd

from torch import nn
from torch.optim import Adam
from tqdm import tqdm
from torch.nn.utils.rnn import pad_sequence
from torchaudio.functional import edit_distance
from torchaudio import transforms as T
from torchaudio.models.decoder import (
    CTCDecoderLM,
    CTCDecoderLMState,
    download_pretrained_files,
    ctc_decoder,
)

# ------------------------------------------------------------------------
#  model classes and helper classes
# ------------------------------------------------------------------------

class CustomLanguageModel(CTCDecoderLM):
    """
    A wrapper around a given language model to provide scoring for CTC decoding.
    """
    def __init__(self, lm_module: torch.nn.Module):
        super().__init__()
        self.lm_module = lm_module
        self.silence_token = -1  # token index reserved for silence
        self._state_cache = {}
        self.lm_module.eval()

    def start(self, ignore_unused: bool = False):
        init_state = CTCDecoderLMState()
        with torch.no_grad():
            score = self.lm_module(self.silence_token)
        self._state_cache[init_state] = score
        return init_state

    def score(self, state: CTCDecoderLMState, token: int):
        new_state = state.child(token)
        if new_state not in self._state_cache:
            score = self.lm_module(token)
            self._state_cache[new_state] = score
        return new_state, self._state_cache[new_state]

    def finish(self, state: CTCDecoderLMState):
        return self.score(state, self.silence_token)


class SimpleCTCDecoder(nn.Module):
    """
    Implements a greedy decoding scheme on a CTC output.
    """
    def __init__(self, symbols, blank_idx=0):
        super(SimpleCTCDecoder, self).__init__()
        self.symbols = symbols
        self.blank_idx = blank_idx

    def forward(self, logits: torch.Tensor) -> list:
        # 1) Most likely symbol at each time step
        best_indices = torch.argmax(logits, dim=-1)
        # 2) Remove consecutive duplicates
        best_indices = torch.unique_consecutive(best_indices, dim=-1)
        # 3) Remove blank tokens and convert to string
        filtered = [i for i in best_indices.flatten().tolist() if i != self.blank_idx]
        decoded_str = "".join([self.symbols[i] for i in filtered])
        # 4) Replace '|' with spaces and split into words
        return decoded_str.replace("|", " ").strip().split()


class AcousticModelWrapper(nn.Module):
    """
    Wraps a pretrained wav2vec2 model with a trainable adapter.
    """
    def __init__(self, num_tokens, *args, **kwargs):
        super().__init__(*args, **kwargs)
        bundle = torchaudio.pipelines.WAV2VEC2_ASR_BASE_960H
        self.feature_extractor = bundle.get_model().to(device)
        # Freeze all parameters of the base model
        for param in self.feature_extractor.parameters():
            param.requires_grad = False
        # Adapter layer to map the 29-dim outputs to our token set
        self.adapter = nn.Linear(29, num_tokens)

    def forward(self, waveform: torch.Tensor):
        with torch.no_grad():
            features, _ = self.feature_extractor(waveform)
        return self.adapter(features)

# ------------------------------------------------------------------------
# helper functions
# ------------------------------------------------------------------------

def get_device():
    """
    Returns GPU device if available, otherwise CPU.
    """
    if torch.cuda.is_available():
        print("Using GPU (CUDA)")
        return torch.device("cuda")
    else:
        print("Using CPU")
        return torch.device("cpu")

def numerical_sort_key(filepath):
    """
    Sort key that tries to extract an integer from the filename;
    if none is found, returns the raw filename.
    """
    base = os.path.basename(filepath)
    match = re.search(r'\d+', base)
    if match:
        return int(match.group())
    return base

def gather_file_paths(folder):
    """
    Recursively collects all file paths from the specified directory.
    Skips .DS_Store if present.
    """
    paths = []
    for root, _, files in os.walk(folder):
        for f in files:
            if f != '.DS_Store':
                paths.append(os.path.join(root, f))
    return paths

def generate_transcript(filename_str, digit2word):
    """
    Given a filename-based string, create a transcript list with '|' between pieces.
    E.g. "abc" -> ["digit2word[a]", "|", "digit2word[b]", "|", "digit2word[c]"]
    """
    transcript_out = []
    for idx, ch in enumerate(filename_str):
        transcript_out.append(digit2word[ch])
        if idx < len(filename_str) - 1:
            transcript_out.append("|")
    return transcript_out

def compute_error_rates(reference, hypothesis):
    """
    Computes Word Error Rate (WER) and Character Error Rate (CER).
    """
    wer_val = edit_distance(reference, hypothesis) / len(reference)
    ref_chars = list("".join(reference))
    hyp_chars = list("".join(hypothesis))
    cer_val = edit_distance(ref_chars, hyp_chars) / len(ref_chars)
    return wer_val, cer_val

def convert_transcript_to_indices(transcript, mapping):
    """
    Converts a transcript list (e.g. ["oh", "|", "one"]) into a tensor of token indices.
    """
    token_ids = []
    for segment in transcript:
        for char in segment:
            token_ids.append(mapping[char])
    tokens_tensor = torch.tensor(token_ids).to(device)
    length_tensor = torch.tensor(len(token_ids), dtype=torch.long).to(device)
    return tokens_tensor, length_tensor


# ------------------------------------------------------------------------
# Global device setup
# ------------------------------------------------------------------------
device = get_device()

# ------------------------------------------------------------------------
# configuration, mappings, and create the model/optimizer
# ------------------------------------------------------------------------

def define_config():
    """
    Returns important hyperparameters and paths.
    """
    DATA_ROOT = '/content' #change it
    FREQ = 16000
    LEARNING_RATE = 0.001
    return DATA_ROOT, FREQ, LEARNING_RATE

def define_mappings():
    """
    Returns character-to-token dict and digit2word dict.
    """
    # Index 0 is reserved for the blank
    char_to_token = {
        "o": 1,
        "h": 2,
        "z": 3,
        "e": 4,
        "r": 5,
        "n": 6,
        "t": 7,
        "w": 8,
        "f": 9,
        "u": 10,
        "i": 11,
        "v": 12,
        "s": 13,
        "x": 14,
        "g": 15,
        "|": 16
    }
    digit2word_map = {
        "o": "oh",
        "z": "zero",
        "1": "one",
        "2": "two",
        "3": "three",
        "4": "four",
        "5": "five",
        "6": "six",
        "7": "seven",
        "8": "eight",
        "9": "nine"
    }
    return char_to_token, digit2word_map

def create_model_and_optimizer(num_tokens, lr):
    """
    Creates the acoustic model, the optimizer, and the CTC loss function.
    """
    model = AcousticModelWrapper(num_tokens).to(device)
    ctc_loss_fn = nn.CTCLoss()
    optimizer = Adam(model.parameters(), lr=lr)
    return model, ctc_loss_fn, optimizer


# ------------------------------------------------------------------------
# Train, Evaluate, and Test
# ------------------------------------------------------------------------

def train_model(model, optimizer, loss_fn, train_files, freq, digit2word_map, char_to_token):
    """
    Trains the model on the given list of training files.
    Returns the array of training loss values.
    """
    model.train()
    train_loss_history = []

    for idx, fpath in enumerate(train_files):
        waveform, sr = torchaudio.load(fpath)
        waveform = waveform.to(device)

        # Resample if needed
        if sr != freq:
            resample = T.Resample(orig_freq=sr, new_freq=freq).to(device)
            waveform = resample(waveform)

        # Create transcript from filename
        base_name = os.path.splitext(os.path.basename(fpath))[0]
        # Remove the last character to form the transcript string
        transcript_str = base_name[:-1]
        transcript_seq = generate_transcript(transcript_str, digit2word_map)
        token_seq, token_len = convert_transcript_to_indices(transcript_seq, char_to_token)

        # Forward + Loss
        optimizer.zero_grad()
        outputs = model(waveform).log_softmax(dim=-1)
        time_steps = torch.tensor([outputs.shape[1]]).to(device)
        loss_val = loss_fn(outputs.permute(1, 0, 2), token_seq, time_steps, token_len)

        # Backward + Optimize
        loss_val.backward()
        optimizer.step()

        train_loss_history.append(loss_val.item())

    return np.array(train_loss_history)


def build_beam_decoders(data_root, symbols_list):
    """
    Creates 6 beam-search decoders (3 with LM, 3 without LM).
    (Greedy is handled separately by SimpleCTCDecoder.)
    """
    # We have 7 total decoders in the original code: 1 greedy + 6 beam decoders
    # So here we just build the 6 beam decoders.
    # If you need to rename or reorder them, adjust accordingly.
    NUM_DECODERS = 6
    beam_sizes = [1, 50, 500]
    LM_WEIGHT = 3.23
    WORD_SCORE = -0.26

    beam_decoders = []
    for i in range(NUM_DECODERS):
        if i < 3:  # With language model
            inst_lm = os.path.join(data_root, "kenlm.arpa")
        else:      # Without language model
            inst_lm = None

        decoder_instance = ctc_decoder(
            nbest=3,
            sil_token='|',
            blank_token="-",
            word_score=WORD_SCORE,
            lm_weight=LM_WEIGHT,
            tokens=symbols_list,
            lexicon=os.path.join(data_root, "lexicon.txt"),
            beam_size=beam_sizes[i % 3],
            lm=inst_lm,
        )
        decoder_instance.decode_begin()
        beam_decoders.append(decoder_instance)

    return beam_decoders


def evaluate_on_validation(model, valid_files, freq, digit2word_map, greedy_decoder, beam_decoders):
    """
    Evaluates the model on the validation set using greedy and all beam decoders.
    Returns (wer_scores, cer_scores) of shape (1 + len(beam_decoders), num_valid_files).
    """
    model.eval()
    num_decoders = 1 + len(beam_decoders)  # 1 for greedy, plus the beam decoders
    wer_scores = np.zeros((num_decoders, len(valid_files)))
    cer_scores = np.zeros((num_decoders, len(valid_files)))

    for file_idx, vpath in enumerate(valid_files):
        waveform, sr = torchaudio.load(vpath)
        waveform = waveform.to(device)

        # Resample if needed
        if sr != freq:
            resample = T.Resample(orig_freq=sr, new_freq=freq).to(device)
            waveform = resample(waveform)

        # Reference transcript
        base = os.path.splitext(os.path.basename(vpath))[0]
        ref_transcript = generate_transcript(base[:-1], digit2word_map)

        # Forward pass
        with torch.no_grad():
            model_output = model(waveform)
            emissions_for_decoding = model_output.log_softmax(dim=-1).cpu()

        # 1) Greedy decoding
        greedy_result = greedy_decoder(model_output)
        if len(greedy_result) > 0:
            # Insert '|' between decoded words
            greedy_with_sep = []
            for i, word in enumerate(greedy_result):
                greedy_with_sep.append(word)
                if i < len(greedy_result) - 1:
                    greedy_with_sep.append("|")
        else:
            greedy_with_sep = []

        wer_greedy, cer_greedy = compute_error_rates(ref_transcript, greedy_with_sep)
        wer_scores[0, file_idx] = wer_greedy
        cer_scores[0, file_idx] = cer_greedy

        # 2) Beam decoders
        for dec_idx, decoder in enumerate(beam_decoders, start=1):
            results = decoder(emissions_for_decoding)
            # results[i] -> [ (Hypothesis object) , (Timesteps) ]
            # The top hypothesis is results[i][0].
            decoded_words = results[0][0].words  # the top hypothesis
            beam_transcript = generate_transcript(decoded_words, digit2word_map)

            wer_val, cer_val = compute_error_rates(ref_transcript, beam_transcript)
            wer_scores[dec_idx, file_idx] = wer_val
            cer_scores[dec_idx, file_idx] = cer_val

    return wer_scores, cer_scores


def print_average_error_rates(wer_scores, cer_scores):
    """
    Prints the average WER/CER for all decoders.
    """
    avg_wer = np.mean(wer_scores, axis=1)
    avg_cer = np.mean(cer_scores, axis=1)

    # 1st row is greedy, then 3 with LM, then 3 without LM
    decoder_names = [
        "Greedy Decoder",
        "CTC Decoder with LM (beam size: 1)",
        "CTC Decoder with LM (beam size: 50)",
        "CTC Decoder with LM (beam size: 500)",
        "CTC Decoder without LM (beam size: 1)",
        "CTC Decoder without LM (beam size: 50)",
        "CTC Decoder without LM (beam size: 500)"
    ]
    # But we only have 1 + 6 = 7 total.

    print("\nAverage WER and CER for each decoder:")
    for i in range(wer_scores.shape[0]):
        print(f"{decoder_names[i]}: WER = {avg_wer[i]:.3f}, CER = {avg_cer[i]:.3f}")


def generate_test_predictions(model, test_files, freq, chosen_decoder, output_filename):
    """
    Runs inference on test_files using chosen_decoder, writes results to output file.
    """
    model.eval()
    with open(output_filename, 'w') as fout:
        for tpath in test_files:
            waveform, sr = torchaudio.load(tpath)
            waveform = waveform.to(device)

            if sr != freq:
                resample = T.Resample(orig_freq=sr, new_freq=freq).to(device)
                waveform = resample(waveform)

            with torch.no_grad():
                emissions = model(waveform).log_softmax(dim=-1).cpu()
            prediction = chosen_decoder(emissions)
            # The top candidate
            pred_text = "".join(prediction[0][0].words)
            fout.write(f"{os.path.basename(tpath)} - {pred_text}\n")

    print(f"Test predictions saved to: {output_filename}")


# ------------------------------------------------------------------------
# 'main'
# ------------------------------------------------------------------------

# Define all config
DATA_ROOT, FREQ, LEARNING_RATE = define_config()

# Define symbol mappings
char_to_token, digit2word_map = define_mappings()

# Build a list of symbols for decoding; index 0 is for blank (represented as '-')
symbols_list = list(char_to_token.keys())
symbols_list.insert(0, '-')

# Create model, loss, optimizer
NUM_TOKENS = len(symbols_list)  # or 17 if you're certain
model, ctc_loss_fn, optimizer = create_model_and_optimizer(NUM_TOKENS, LEARNING_RATE)

# Gather training and validation files
all_train_files = gather_file_paths(os.path.join(DATA_ROOT, "train"))
split_ratio = 0.8
total_files = len(all_train_files)
num_train = round(split_ratio * total_files)
train_files = all_train_files[:num_train - 1]
valid_files = all_train_files[num_train:-1]

# Training
train_loss_history = train_model(
    model, optimizer, ctc_loss_fn,
    train_files, FREQ, digit2word_map, char_to_token
)
print("Finished training. Saving model...")
torch.save(model.state_dict(), 'my_model.pt')

# Evaluate on validation set
greedy_decoder = SimpleCTCDecoder(symbols_list)
beam_decoders = build_beam_decoders(DATA_ROOT, symbols_list)
wer_scores, cer_scores = evaluate_on_validation(
    model, valid_files, FREQ,
    digit2word_map, greedy_decoder, beam_decoders
)
print_average_error_rates(wer_scores, cer_scores)

# Choose a decoder for test
chosen_decoder = beam_decoders[4]

# Generate predictions on test set
test_files = sorted(gather_file_paths(os.path.join(DATA_ROOT, "test")), key=numerical_sort_key)
generate_test_predictions(model, test_files, FREQ, chosen_decoder, 'output.txt')